# -*- coding: utf-8 -*-
"""IRS990_Scrape.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SocJOU-e2LfSxvue2P_keBJ7TLwwpkZm
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import requests # Getting Webpage content
from bs4 import BeautifulSoup as bs # Scraping webpages
import matplotlib.pyplot as plt # Visualization
import matplotlib.style as style # For styling plots
from matplotlib import pyplot as mp # For Saving plots as images
import json
import nltk
nltk.download()

# For displaying plots in jupyter notebook
# %matplotlib inline 

style.use('fivethirtyeight') # matplotlib Style

"""###Testing"""

#"EIN":"562618866","TaxPeriod":"201512","DLN":"93491316008226","FormType":"990PF","URL":"https://s3.amazonaws.com/irs-form-990/201623169349100822_public.xml","OrganizationName":"BILL AND MELINDA GATES FOUNDATION","SubmittedOn":"2017-03-30","ObjectId":"201623169349100822","LastUpdated":"2017-04-10T21:32:14"}
#https://s3.amazonaws.com/irs-form-990/201941489349100000_public.xml

test_EIN = '562618866'
test_990url = ''
for entry in data2019['Filings2019']:
  if entry['EIN'] == test_EIN:
    test_990url = entry['URL']
    break

test_response = requests.get(test_990url)
test_response.status_code

soup = bs(test_response.content, 'html.parser')
f = open('test_990.txt', 'w')
f.write(str(soup))
f.close()

"""#Full foundation list"""

full_foundation_list = pd.read_csv('UT Full Foundation List .csv')
full_foundation_list.head()

full_foundation_list=full_foundation_list.astype(str)

"""##2020"""

# Using requests module for downloading webpage content
response2020 = requests.get('https://s3.amazonaws.com/irs-form-990/index_2020.json')
# Getting status of the request
# 200 status code means our request was successful
# 404 status code means that the resource you were looking for was not found
response2020.status_code

# Parsing html data using BeautifulSoup
soup2020 = bs(response2020.content, 'html.parser')

#Saving 990 index files as txt files for 990 returns 2020

f=open('index_2020.txt', 'w')
f.write(str(soup2020))
f.close()

#Saving summaries of 990 files (from index files) 2020
data_2020=''
summary_2020=''
with open('index_2020.txt') as json_file: 
    data_2020 = json.load(json_file)
summary_2020 = data_2020['Filings2020']
print(len(summary_2020))

print(summary_2020[0])

#Declaring variables (Data to be collected after matching with full list)

ObjectId = []
EIN = []
DLN = []
OrganizationName = []
URL = []

unmatched = []

index = dict()

for entry in data_2020['Filings2020']:
  name = entry['OrganizationName'].lower()
  index[name] = entry

matched = dict()
unmatched = []

for row in full_foundation_list['NAME']:
  row = row.lower()
  keys = index.keys()
  if row in keys:
    matched[row] = index[row]
  else:
    unmatched.append(row)

print(unmatched)

print(len(matched.keys()))

print(json.dumps(matched, indent=4))

import requests

response = requests.get("https://s3.amazonaws.com/irs-form-990/201913129349100421_public.xml")
print(response.text)

"""#Retrieving 990 forms (2020)"""

with open('full_foundation_list_data.json', 'w') as fp:
  json.dump(matched, fp, indent = 4)

print(matched['woodward family foundation']['URL'])

response = requests.get(matched['woodward family foundation']['URL'])
response.status_code

#creating 990 forms dataset
full_foundation_list_990 = {}

for k in matched:
  response = requests.get(matched[k]['URL'])
  full_foundation_list_990[k] = response.text

with open('full_foundation_list_990.json', 'w') as fp:
  json.dump(full_foundation_list_990, fp, indent = 4)

"""#Fuzzy Matching Method 1

##Cleaning index dataset and identifying the most commonly used words
"""

def clean_special_characters(txt):
  seps = [" ", ";", ":", ",", ".", "*", "#", "@", "|", "\\", "-", "_", "!", "%", "^", "(", ")"]
  default_sep = seps[0]

  for sep in seps[1:]:
    txt = txt.replace(sep, default_sep)
  #re.sub(' +', ' ', txt)
  temp_list = [i.strip() for i in txt.split(default_sep)]
  temp_list = [i for i in temp_list if i]
  return " ".join(temp_list)

def clean_stopword(txt):
  
  temp_list = txt.split(" ")
  temp_list = [i for i in temp_list if i not in stopwords]
  return " ".join(temp_list)

def data_cleaning(data, nameCol='CompanyName', dropForeign = True):   #colName for company name - create df?
  data.dropna(subset = [nameCol], inplace=True)
  #data = data.rename_axis('CompanyID').resetIndex()
  data['nonAscii_count'] = data[nameCol].apply(lambda x: sum([not c.isascii() for c in x]))
  if dropForeign:
    data=data[data.nonAscii_count==0]
  else:
    pass
  data.drop('nonAscii_count', axis=1, inplace=True)
  data_clean=data.copy()
  data_clean['Name_clean'] = data_clean[nameCol].apply(lambda x: x.lower())
  data_clean['Name_clean'] = data_clean['Name_clean'].apply(clean_special_characters)
  data_clean['Name_clean'] = data_clean['Name_clean'].apply(clean_stopword)
  return data_clean

"""##Similarity score, Clustering similar names, Assigning Standard names, Confidence Score, White space correction"""

def similarity(company_names):
  company_names = company_names.split()
  similarity_array = np.ones((len(company_names),(len(company_names))))*100

  for i in range(1, len(company_names)):
    for j in range(i):
      s1 = fuzz.token_set_ratio(company_names[i], company_names[j])+0.000000000001
      s2 = fuzz.partial_ratio(company_names[i], company_names[j]) + 0.00000000001
      similarity_array[i][j] = 2*s1*s2 / (s1+s2)
  
  for i in range(len(company_names)):
    for j in range(i+1, len(company_names)):
      similarity_array[i][j]

  np.fill_diagonal(similarity_array, 100)
  return  similarity_array


def company_clusters(data, nameCol='CompanyName', dropForeign=True):
  data_clean = data_cleaning(data, nameCol=nameCol, dropForeign=dropForeign)
  company_names = data_clean.CustomerName_clean.to_list()
  #cust_ids = data_clean.companyID.to_list()

  similarity_array = fuzz_similarity(company_names)
  clusters = cluster.AffinityPropagation(affinity='precomputed').fit_predict(similarity_array)
  df_clusters = pd.DataFrame(list(zip(cust_ids, clusters)), columns=['CompanyID', 'cluster'])

  df_eval = df_clusters.merge(data_clean, on='CompanyID', how='left')
  return df_eval

def standard_name(df_eval):
  d_standard_name={}
  for cluster in df_eval.cluster.unique():
    names = df_eval[df_eval['cluster']==cluster].CompanyName_clean.to_list()
    l_common_substring = []
    if len(names)>1:
      for i in range(0,len(names)):
        for j in range(i+1, len(names)):
          seqMatch = SequenceMatcher(None, names[i],names[j])
          match = seqMatch.find_longest_match(0, len(names[i]), 0, names[j])
          if match.size!=0:
            l_common_substring.append(names[i][match.a: match.a + match.size].strip())
    
    else:
      d_standard_name[cluster] = names[0]
  
  df_standard_names = pd.DataFrame(list(d_standard_name.items()), columns=['cluster', 'StandardName'])
  df_eval = df_eval.merge(df.standard_names, on='cluster', how='left')
  df_eval['Score_with_standard'] = df_eval.apply(lambda x: fuzz.token_set_ratio(x['StandardName'], x[CompanyName_clean]), axis=1)
  df_eval['standard_name_withoutSpaces'] = df_eval.StandardName.apply(lambda x: x.replace(' ', ''))

  for name in df_eval.standard_name_withoutSpaces.unique():
    if len(df_eval[df_eval.standard_name_withoutSpaces==name].cluster.unique()) > 1:
      df_eval.loc[df_eval.standard_name_withoutSpaces==name, 'StandardName'] = name
  
  return df_eval.drop('standard_name_withoutSpaces', axis = 1)

"""#Fuzzy Matching Method 2"""

#!pip3 install fuzzywuzzy
from fuzzywuzzy import fuzz

def match_name(name, list_names, min_score=0):

  #if there are no matches, return -1
  max_score = -1    
  #returning empty name if no match
  max_name=''
  for name2 in list_names:
    score = fuzz.ratio(name,name2)

    if (score>min_score) and (score>max_score):
      max_name = name2
      max_score = score
  
  return (max_name, max_score)

#ut_list_names = list(unmatched.keys())
ut_list_names = unmatched
ut_list_names[:10]

index_names = list(index.keys())
print(len(index_names))

index_dict = {}
for name in index_names:
  index_dict[name] = index[name]['URL']

from nltk.corpus import stopwords
from fuzzywuzzy import fuzz

def clean_stopword(txt):
  stop_words = set(stopwords.words('english'))
  extended_stopwords = set([key for key, _ in wordfreq])
  updated_stop_words = stop_words | extended_stopwords
  temp_list = txt.split(" ")
  temp_list = [i for i in temp_list if i not in updated_stop_words]
  return " ".join(temp_list)

for key in index_dict.keys():
  #index_names[i] = clean_special_characters(index_names[i])
  clean_key = clean_stopword(key)
  index_wo_stopwords_dict[clean_key] = index_dict[key]

print(index_wo_stopwords_dict['quincy arena'])

dict_list = []

for name2search in ut_list_names:
  match = match_name(name2search, index_names, 75)

  dict_ = {}
  dict_.update({"company name" : name2search})
  dict_.update({"match_name" : match[0]})
  dict_.update({"score" : match[1]})
  dict_list.append(dict_)

merge_table = pd.DataFrame(dict_list)
merge_table

merge_table = pd.DataFrame(dict_list)
merge_table

"""#Fuzzy Matching Method 3"""

from collections import Counter

index_names = list(index.keys())
listToStr_unmatched = ' '.join(map(str, unmatched))
listToStr_index = ' '.join(map(str, index_names))

listToStr = listToStr_unmatched + ' ' + listToStr_index

wordlist = listToStr.split()

wordfreq = Counter(wordlist).most_common(100)
#print wordfreq.most_common(20)

index_wo_stopwords_dict = {}

print(wordfreq)

from google.colab import files
!pip3 install fuzzywuzzy

from nltk.corpus import stopwords
from fuzzywuzzy import fuzz

def clean_special_characters(txt):
  seps = [" ", ";", ":", ",", ".", "*", "#", "@", "|", "\\", "-", "_", "!", "%", "^", "(", ")"]
  default_sep = seps[0]
  for sep in seps[1:]:
    txt = txt.replace(sep, default_sep)
  #re.sub(' +', ' ', txt)
  temp_list = [i.strip() for i in txt.split(default_sep)]
  temp_list = [i for i in temp_list if i]
  return " ".join(temp_list)


def clean_stopword(txt):
  stop_words = set(stopwords.words('english'))
  extended_stopwords = set([key for key, _ in wordfreq])
  updated_stop_words = stop_words | extended_stopwords
  temp_list = txt.split(" ")
  temp_list = [i for i in temp_list if i not in updated_stop_words]
  return " ".join(temp_list)


def match_name(name, list_names, min_score=0):

  #if there are no matches, return -1
  max_score = -1    
  #returning empty name if no match
  max_name=''
  for name2 in list_names:
    score = fuzz.token_set_ratio(name,name2)
    if (score>min_score) and (score>max_score):
      max_name = name2
      max_score = score
  return (max_name, max_score)


ut_list_names = unmatched
index_names = list(index.keys())
for i in range(len(ut_list_names)):
  #ut_list_names[i] = clean_special_characters(ut_list_names[i])
  ut_list_names[i] = clean_stopword(ut_list_names[i])

for key in index_dict.keys():
  #index_names[i] = clean_special_characters(index_names[i])
  clean_key = clean_stopword(key)
  index_wo_stopwords_dict[clean_key] = index_dict[key]

dict_list = []
merge_table = pd.DataFrame()
flag=0
for name2search in ut_list_names:
  match = match_name(name2search, index_names, 75)
  if(flag%100==0):
    print(flag)
  flag+=1
  dict_ = {}
  dict_.update({"company name" : name2search})
  dict_.update({"match_name" : match[0]})
  dict_.update({"score" : match[1]})
  dict_list.append(dict_)
  merge_table.append(dict_list, ignore_index = True)


merge_table.to_csv('merge_table.csv', sep='\t', encoding='utf-8')
files.download('merge_table.csv')

print(dict_list)

import tensorflow as tf
tf.test.gpu_device_name()

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import requests # Getting Webpage content
from bs4 import BeautifulSoup as bs # Scraping webpages
import json
import nltk
nltk.download()
from collections import Counter
from nltk.corpus import stopwords
from fuzzywuzzy import fuzz

#full foundation list
full_foundation_list = pd.read_csv('UT Full Foundation List .csv')
full_foundation_list.head()

full_foundation_list=full_foundation_list.astype(str)

#downloading 2020 index
# Using requests module for downloading webpage content
response2020 = requests.get('https://s3.amazonaws.com/irs-form-990/index_2020.json')
# Getting status of the request
# 200 status code means our request was successful
# 404 status code means that the resource you were looking for was not found
response2020.status_code

# Parsing html data using BeautifulSoup
soup2020 = bs(response2020.content, 'html.parser')

#Saving 990 index files as txt files for 990 returns 2020

f=open('index_2020.txt', 'w')
f.write(str(soup2020))
f.close()

#Saving summaries of 990 files (from index files) 2020
data_2020=''
summary_2020=''
with open('index_2020.txt') as json_file: 
    data_2020 = json.load(json_file)
summary_2020 = data_2020['Filings2020']
print(len(summary_2020))

index = dict()

for entry in data_2020['Filings2020']:
  name = entry['OrganizationName'].lower()
  index[name] = entry

matched = dict()
unmatched = []

for row in full_foundation_list['NAME']:
  row = row.lower()
  keys = index.keys()
  if row in keys:
    matched[row] = index[row]
  else:
    unmatched.append(row)

index_names = list(index.keys())
print(len(index_names))

index_dict = {}
for name in index_names:
  index_dict[name] = index[name]['URL']


#creating stopword list


index_names = list(index.keys())
listToStr_unmatched = ' '.join(map(str, unmatched))
listToStr_index = ' '.join(map(str, index_names))

listToStr = listToStr_unmatched + ' ' + listToStr_index

wordlist = listToStr.split()

wordfreq = Counter(wordlist).most_common(100)
#print wordfreq.most_common(20)

index_wo_stopwords_dict = {}

#cleaning stopwords
def clean_stopword(txt):
  stop_words = set(stopwords.words('english'))
  extended_stopwords = set([key for key, _ in wordfreq])
  updated_stop_words = stop_words | extended_stopwords
  temp_list = txt.split(" ")
  temp_list = [i for i in temp_list if i not in updated_stop_words]
  return " ".join(temp_list)

#fuzzy matching
def match_name(name, list_names, min_score=0):

  #if there are no matches, return -1
  max_score = -1    
  #returning empty name if no match
  max_name=''
  for name2 in list_names:
    score = fuzz.token_set_ratio(name,name2)
    if (score>min_score) and (score>max_score):
      max_name = name2
      max_score = score
  return (max_name, max_score)

ut_list_names = unmatched
index_names = list(index.keys())
for i in range(len(ut_list_names)):
  ut_list_names[i] = clean_stopword(ut_list_names[i])

index_names=[]
for key in index_dict.keys():
  clean_key = clean_stopword(key)
  index_names.append(clean_key)
  index_wo_stopwords_dict[clean_key] = index_dict[key]

dict_list = []

merge_table = pd.DataFrame()
flag=0
ctr = 1
#ut_list_names_RD = ut_list_names[1100:]
for name2search in ut_list_names:
  match = match_name(name2search, index_names, 75)
  if(flag%2==0):
    merge_table = merge_table.append(dict_list, ignore_index = True)
    dict_list = []
    fname = 'merge_table_'+str(ctr)+'.csv'
    merge_table.to_csv(fname, sep='\t', encoding='utf-8')
    !cp $fname "/content/drive/My Drive/"
    ctr+=1
  if(flag%5==0):
    break
  flag+=1
  dict_ = {}
  dict_.update({"company name" : name2search})
  dict_.update({"match_name" : match[0]})
  dict_.update({"score" : match[1]})
  dict_.update({"URL" : index_wo_stopwords_dict[match[0]]})
  dict_list.append(dict_)

fname = 'merge_table_'+str(ctr)+'.csv'
merge_table = merge_table.append(dict_list, ignore_index = True)
merge_table.to_csv('merge_table_'+str(ctr)+'.csv', sep='\t', encoding='utf-8')
!cp $fname.csv "/content/drive/My Drive/"

from google.colab import drive
drive.mount('/content/drive')

print(dict_)

"""#Matched data analysis and 990 retrieval"""

fuzzy_df = pd.read_csv('merge_table.csv', sep='\\t')
fuzzy_df.head()

name1, name2, url = [], [], []

for index, row in fuzzy_df.iterrows():  
  if row['score']==100:
    name1.append(str(row['company name']))
    name2.append(str(row['match_name']))
    url.append(str(row['URL"']))

"""##Extracting perfect name matches (after removing stopwords) first"""

perfect_match_name = []
perfect_match_url = []

for i in range(len(name1)):
  if (name1[i] == name2[i]):
    perfect_match_name.append(name1[i])
    perfect_match_url.append(url[i])

print(len(perfect_match_url))

print(perfect_match_name)

